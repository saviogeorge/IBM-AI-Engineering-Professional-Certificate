{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://cognitiveclass.ai\"><img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\"> </a>\n",
    "\n",
    "<h1 align=center><font size = 5>Pre-Trained Models</font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will learn how to leverage pre-trained models to build image classifiers instead of building a model from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. <a href=\"#item31\">Import Libraries and Packages</a>\n",
    "2. <a href=\"#item32\">Download Data</a>  \n",
    "3. <a href=\"#item33\">Define Global Constants</a>  \n",
    "4. <a href=\"#item34\">Construct ImageDataGenerator Instances</a>  \n",
    "5. <a href=\"#item35\">Compile and Fit Model</a>\n",
    "\n",
    "</font>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item31'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Packages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start the lab by importing the libraries that we will be using in this lab. First we will need the library that helps us to import the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skillsnetwork "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will import the ImageDataGenerator module since we will be leveraging it to train our model in batches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will be using the Keras library to build an image classifier, so let's download the Keras library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will be leveraging the ResNet50 model to build our classifier, so let's download it as well.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item32'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you are going to download the data from IBM object storage using **skillsnetwork.prepare** command. skillsnetwork.prepare is a command that's used to download a zip file, unzip it and store it in a specified directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fea6a3383dd944838cf286833f1110a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading concrete_data_week3.zip:   0%|          | 0/97863179 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e37ce4a3a24a4e6d961dde80ad8c5f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/30036 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "## get the data\n",
    "await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/concrete_data_week3.zip\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the folder *concrete_data_week3* appear in the left pane. If you open this folder by double-clicking on it, you will find that it contains two folders: *train* and *valid*. And if you explore these folders, you will find that each contains two subfolders: *positive* and *negative*. These are the same folders that we saw in the labs in the previous modules of this course, where *negative* is the negative class and it represents the concrete images with no cracks and *positive* is the positive class and it represents the concrete images with cracks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: There are thousands and thousands of images in each folder, so please don't attempt to double click on the *negative* and *positive* folders. This may consume all of your memory and you may end up with a **50** error. So please **DO NOT DO IT**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item33'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Global Constants\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will define constants that we will be using throughout the rest of the lab. \n",
    "\n",
    "1. We are obviously dealing with two classes, so *num_classes* is 2. \n",
    "2. The ResNet50 model was built and trained using images of size (224 x 224). Therefore, we will have to resize our images from (227 x 227) to (224 x 224).\n",
    "3. We will training and validating the model using batches of 100 images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "image_resize = 224\n",
    "\n",
    "batch_size_training = 100\n",
    "batch_size_validation = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item34'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct ImageDataGenerator Instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to instantiate an ImageDataGenerator instance, we will set the **preprocessing_function** argument to *preprocess_input* which we imported from **keras.applications.resnet50** in order to preprocess our images the same way the images used to train ResNet50 model were processed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_generator = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the *flow_from_directory* method to get the training images as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/train',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_training,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: in this lab, we will be using the full data-set of 30,000 images for training and validation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your Turn**: Use the *flow_from_directory* method to get the validation images and assign the result to **validation_generator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5001 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## Type your answer here\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Double-click __here__ for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = data_generator.flow_from_directory(\n",
    "    'concrete_data_week3/valid',\n",
    "    target_size=(image_resize, image_resize),\n",
    "    batch_size=batch_size_validation,\n",
    "    class_mode='categorical')\n",
    "-->\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='item35'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build, Compile and Fit Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will start building our model. We will use the Sequential model class from Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will add the ResNet50 pre-trained model to out model. However, note that we don't want to include the top layer or the output layer of the pre-trained model. We actually want to define our own output layer and train it so that it is optimized for our image dataset. In order to leave out the output layer of the pre-trained model, we will use the argument *include_top* and set it to **False**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3837: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3661: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 06:26:26.824186: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
      "2024-09-09 06:26:26.831128: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2394330000 Hz\n",
      "2024-09-09 06:26:26.831775: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x56246cc1b370 executing computations on platform Host. Devices:\n",
      "2024-09-09 06:26:26.831829: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2024-09-09 06:26:26.870775: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3665: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
      "\n",
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.2/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "94658560/94653016 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "model.add(ResNet50(\n",
    "    include_top=False,\n",
    "    pooling='avg',\n",
    "    weights='imagenet',\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will define our output layer as a **Dense** layer, that consists of two nodes and uses the **Softmax** function as the activation function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the model's layers using the *layers* attribute of our model object. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.training.Model at 0x7fc0c6573f10>,\n",
       " <keras.layers.core.Dense at 0x7fc0c46efe90>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that our model is composed of two sets of layers. The first set is the layers pertaining to ResNet50 and the second set is a single layer, which is our Dense layer that we defined above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the ResNet50 layers by running the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fc15394cf90>,\n",
       " <keras.layers.convolutional.ZeroPadding2D at 0x7fc147beced0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc1816ac410>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc14dc63ad0>,\n",
       " <keras.layers.core.Activation at 0x7fc1817ab750>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fc1545233d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc14d21d250>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc14d133e50>,\n",
       " <keras.layers.core.Activation at 0x7fc14d133590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc14d148290>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc14d0b3f10>,\n",
       " <keras.layers.core.Activation at 0x7fc14d0b3e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc1443b8e90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc1442b25d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc1442d63d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc14426d210>,\n",
       " <keras.layers.merge.Add at 0x7fc14426dd10>,\n",
       " <keras.layers.core.Activation at 0x7fc14413ad10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc144103910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc1440c9a90>,\n",
       " <keras.layers.core.Activation at 0x7fc1441341d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc14404bb50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc124787150>,\n",
       " <keras.layers.core.Activation at 0x7fc124787310>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc124720750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc124669e90>,\n",
       " <keras.layers.merge.Add at 0x7fc1246278d0>,\n",
       " <keras.layers.core.Activation at 0x7fc1245bcad0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc1245ed8d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc12459dfd0>,\n",
       " <keras.layers.core.Activation at 0x7fc12459d690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc124537dd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc1244b0150>,\n",
       " <keras.layers.core.Activation at 0x7fc1244b0210>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc1243ce050>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc124346990>,\n",
       " <keras.layers.merge.Add at 0x7fc1243630d0>,\n",
       " <keras.layers.core.Activation at 0x7fc1242e8c10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc124299850>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc12425e0d0>,\n",
       " <keras.layers.core.Activation at 0x7fc12424a690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc1241e2e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc12412bb50>,\n",
       " <keras.layers.core.Activation at 0x7fc12415b8d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc1240d3990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc10c795f10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc124071810>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc10c749c90>,\n",
       " <keras.layers.merge.Add at 0x7fc10c749650>,\n",
       " <keras.layers.core.Activation at 0x7fc10c6a75d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc10c5f0f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc10c622790>,\n",
       " <keras.layers.core.Activation at 0x7fc10c622e10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc10c546d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc10c4ce190>,\n",
       " <keras.layers.core.Activation at 0x7fc10c527690>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc10c43ead0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc10c42bb10>,\n",
       " <keras.layers.merge.Add at 0x7fc10c42bf10>,\n",
       " <keras.layers.core.Activation at 0x7fc10c357150>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc10c377b50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc10c2bb4d0>,\n",
       " <keras.layers.core.Activation at 0x7fc10c2cffd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc10c270b90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc10c18b3d0>,\n",
       " <keras.layers.core.Activation at 0x7fc10c1d3f50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc10c16bed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc10c0b3a50>,\n",
       " <keras.layers.merge.Add at 0x7fc10c0e95d0>,\n",
       " <keras.layers.core.Activation at 0x7fc0ec7c2190>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0ec753090>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0ec73a4d0>,\n",
       " <keras.layers.core.Activation at 0x7fc0ec73af90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0ec6e0d50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0ec678390>,\n",
       " <keras.layers.core.Activation at 0x7fc0ec7c2810>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0ec5d6590>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0ec540f10>,\n",
       " <keras.layers.merge.Add at 0x7fc0ec540dd0>,\n",
       " <keras.layers.core.Activation at 0x7fc0ec4ee550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0ec4ee7d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0ec451a10>,\n",
       " <keras.layers.core.Activation at 0x7fc0ec467d50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0ec38abd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0ec312e50>,\n",
       " <keras.layers.core.Activation at 0x7fc0ec368510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0ec281dd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0ec19c390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0ec26df50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0ec150c10>,\n",
       " <keras.layers.merge.Add at 0x7fc0ec150f10>,\n",
       " <keras.layers.core.Activation at 0x7fc0ec0a1d10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0ec069910>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c7fe4f50>,\n",
       " <keras.layers.core.Activation at 0x7fc0c7fe4550>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7f15890>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c7ef6d90>,\n",
       " <keras.layers.core.Activation at 0x7fc0c7ef6a50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7e16c50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c7d8d490>,\n",
       " <keras.layers.merge.Add at 0x7fc0c7d8d750>,\n",
       " <keras.layers.core.Activation at 0x7fc0c7d29910>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7d29bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c7c9fb10>,\n",
       " <keras.layers.core.Activation at 0x7fc0c7c9fe50>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7c3d350>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c7b05790>,\n",
       " <keras.layers.core.Activation at 0x7fc0c7bbe5d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7ad72d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c7ab9b10>,\n",
       " <keras.layers.merge.Add at 0x7fc0c7ab9450>,\n",
       " <keras.layers.core.Activation at 0x7fc0c79d6cd0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7c3db90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c794e790>,\n",
       " <keras.layers.core.Activation at 0x7fc0c794ef10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7888c10>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c7810fd0>,\n",
       " <keras.layers.core.Activation at 0x7fc0c786a590>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7781bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c776ef50>,\n",
       " <keras.layers.merge.Add at 0x7fc0c776eed0>,\n",
       " <keras.layers.core.Activation at 0x7fc0c76981d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7888390>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c767d990>,\n",
       " <keras.layers.core.Activation at 0x7fc0c7610750>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c75b08d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c752aed0>,\n",
       " <keras.layers.core.Activation at 0x7fc0c74cb050>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7445bd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c73e2450>,\n",
       " <keras.layers.merge.Add at 0x7fc0c742cad0>,\n",
       " <keras.layers.core.Activation at 0x7fc0c7344c10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7344f90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c72c4f10>,\n",
       " <keras.layers.core.Activation at 0x7fc0c725bed0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c7200e50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c71d3310>,\n",
       " <keras.layers.core.Activation at 0x7fc0c71d3450>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c71709d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c70eb5d0>,\n",
       " <keras.layers.merge.Add at 0x7fc0c7088410>,\n",
       " <keras.layers.core.Activation at 0x7fc0c700ce90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c703ca90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c6ff1b50>,\n",
       " <keras.layers.core.Activation at 0x7fc0c6ff16d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c6f21690>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c6e82bd0>,\n",
       " <keras.layers.core.Activation at 0x7fc0c6e82510>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c6e3a9d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c6cd7ed0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c6d9b310>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c6cf1250>,\n",
       " <keras.layers.merge.Add at 0x7fc0c6cf1390>,\n",
       " <keras.layers.core.Activation at 0x7fc0c6bcc490>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c6b6b750>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c6b48b10>,\n",
       " <keras.layers.core.Activation at 0x7fc0c6b48f90>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c6ae57d0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c6a4ef90>,\n",
       " <keras.layers.core.Activation at 0x7fc0c6a4ee10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c69fdd90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c695d890>,\n",
       " <keras.layers.merge.Add at 0x7fc0c695ddd0>,\n",
       " <keras.layers.core.Activation at 0x7fc0c68fdb10>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c68a2fd0>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c6878050>,\n",
       " <keras.layers.core.Activation at 0x7fc0c6878990>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c67adb50>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c670e590>,\n",
       " <keras.layers.core.Activation at 0x7fc0c670e610>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fc0c66a8d90>,\n",
       " <keras.layers.normalization.BatchNormalization at 0x7fc0c6612f10>,\n",
       " <keras.layers.merge.Add at 0x7fc0c6612950>,\n",
       " <keras.layers.core.Activation at 0x7fc0c6541290>,\n",
       " <keras.layers.pooling.AveragePooling2D at 0x7fc0c68fde90>,\n",
       " <keras.layers.pooling.GlobalAveragePooling2D at 0x7fc14d20bc50>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the ResNet50 model has already been trained, then we want to tell our model not to bother with training the ResNet part, but to train only our dense output layer. To do that, we run the following.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now using the *summary* attribute of the model, we can see how many parameters we will need to optimize in order to train the output layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 2048)              23587712  \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we compile our model using the **adam** optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we are able to start the training process, with an ImageDataGenerator, we will need to define how many steps compose an epoch. Typically, that is the number of images divided by the batch size. Therefore, we define our steps per epoch as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "steps_per_epoch_training = len(train_generator)\n",
    "steps_per_epoch_validation = len(validation_generator)\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are ready to start training our model. Unlike a conventional deep learning training were data is not streamed from a directory, with an ImageDataGenerator where data is augmented in batches, we use the **fit_generator** method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyterlab/conda/envs/python/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 06:26:48.144468: W tensorflow/core/framework/allocator.cc:107] Allocation of 321126400 exceeds 10% of system memory.\n",
      "2024-09-09 06:26:52.582740: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-09-09 06:26:58.602365: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-09-09 06:27:04.662177: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n",
      "2024-09-09 06:27:10.708403: W tensorflow/core/framework/allocator.cc:107] Allocation of 309760000 exceeds 10% of system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 24/101 [======>.......................] - ETA: 1:27:43 - loss: 0.2566 - acc: 0.9096"
     ]
    }
   ],
   "source": [
    "fit_history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=steps_per_epoch_training,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=steps_per_epoch_validation,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is trained, you are ready to start using it to classify images.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since training can take a long time when building deep learning models, it is always a good idea to save your model once the training is complete if you believe you will be using the model again later. You will be using this model in the next module, so go ahead and save your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('classifier_resnet_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you should see the model file *classifier_resnet_model.h5* apprear in the left directory pane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for completing this lab!\n",
    "\n",
    "This notebook was created by Alex Aklson. I hope you found this lab interesting and educational.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of a course on **Coursera** called *AI Capstone Project with Deep Learning*. If you accessed this notebook outside the course, you can take this course online by clicking [here](https://cocl.us/DL0321EN_Coursera_Week3_LAB1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2020-09-18  | 2.0  | Shubham  |  Migrated Lab to Markdown and added to course repo in GitLab |\n",
    "| 2023-01-03  | 3.0  | Artem |  Updated the file import section|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "Copyright &copy; 2020 [IBM Developer Skills Network](https://cognitiveclass.ai/?utm_source=bducopyrightlink&utm_medium=dswb&utm_campaign=bdu). This notebook and its source code are released under the terms of the [MIT License](https://bigdatauniversity.com/mit-license/).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "prev_pub_hash": "cf2970a1d2c549fe86023eaa076d0ce4936c4275baf2cccfdad8fe6ce3a8a6c2"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
